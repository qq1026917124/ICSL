---
run_name: xx-xx-Train-Maze-DL-stage0
master_port: "12413"
load_model_path:  "./checkpoints-vae-pro-6/ckpt_70/"  # cold start with new VAE trained by Maze and Prochor Thor

log_config:
  use_tensorboard: True
  tensorboard_log: "./train-record/causal-decoder-wm-DL-stage0/"
  training_log: ./train-record/causal-decoder-wm-DL-stage0/train.log
  evaluation_log: ./train-record/causal-decoder-wm-DL-stage0/eval.log

model_config:
  context_warmup: 1000
  max_position_loss_weighting: 32000
  vae_latent_size: 1024
  action_dim: 16
  policy_loss_type: CrossEntropy
  
  image_encoder_block:
      img_size: 128
      hidden_size: 1024
      n_res_block: 2

  image_decoder_block:
      input_size: 1024
      hidden_size: 1024
      img_size: 128
      n_res_block: 2

  decision_block:
      rsa_type: podaw  #potar, otar, oar, oa
      is_generate: False
      state_encode:
          input_type: Continuous
          input_size: 1024
          hidden_size: 1024
          dropout: 0.0

      action_encode:
          input_type: Discrete
          input_size: 17
          hidden_size: 1024
          dropout: 0.0

      prompt_encode:
          input_type: Continuous
          input_size: 768
          hidden_size: 1024
          dropout: 0.0
          is_frozen: False

      tag_encode:
          input_type: Discrete
          input_size: 8
          hidden_size: 1024
          dropout: 0.0
          is_frozen: False

      state_decode:
          output_type: Continuous
          input_size: 1024
          hidden_size:
              - 1024
              - 1024
          layer_norm: True
          residual_connect: True
          dropout: 0.0

      action_decode:
          output_type: Discrete
          input_size: 1024
          hidden_size:
              - 1024
              - 17
          layer_norm: True
          residual_connect: True
          dropout: 0.0
          
          
      action_diffusion:
          enable: False
          prediction_type: "sample" # epslion / sample /velocity
          diffusion_model_name: "latentlm" # latentlm / basic
          basic_model:
            hidden_size: 1024
            condition_size: 1024
            t_embedding_size: 32
            inner_hidden_size: 4096
            cond_layer_norm: True
            dropout: 0.1
          latentlm:
            hidden_size: 1024
            block_size: 3
            mlp_ratio: 4
            dropout: 0.0
          schedule: "cosine" # linear / cosine / scaled_linear
          T: 20
          beta: [0.0001, 0.02] # For linear / scaled_linear
          inference_sample_steps: 20
          eta: 1.0
          need_clip: False # Set True if predict type is epslion or velocity
          clip_threshold: 5.0
  
      state_diffusion:
          enable: False
          prediction_type: "sample" # epslion / sample / velocity
          diffusion_model_name: "latentlm" # latentlm / basic
          basic_model:
            hidden_size: 1024
            condition_size: 1024
            t_embedding_size: 32
            inner_hidden_size: 4096
            cond_layer_norm: True
            dropout: 0.1
          latentlm:
            hidden_size: 1024
            block_size: 5
            mlp_ratio: 6
            dropout: 0.0
          schedule: "cosine" # linear / cosine / scaled_linear
          T: 20
          beta: [0.0001, 0.02] # For linear / scaled_linear
          inference_sample_steps: 20
          eta: 1.0
          need_clip: False # Set True if predict type is epslion or velocity
          clip_threshold: 5.0

      causal_block_trans:
          model_type: TRANSFORMER
          num_layers: 18
          hidden_size: 1024
          nhead: 32
          inner_hidden_size: 1024
          dropout: 0.10
          context_window: -1
          checkpoints_density: -1
          position_encoding_size: 2048
          use_layer_norm: True
          use_blockrecurrence: True
          memory_length: 1024
          memory_type: KV
          
          
      causal_block:
          is_generate: False
          model_type: GSA # GLA
          num_layers: 18
          hidden_size: 1024
          inner_hidden_size: 1024
          dropout: 0.10
          nhead: 32
          memory_length: 64
          position_encoding_size: 2048
          use_layer_norm: True
          gate_bound: 22
          use_blockrecurrence: True
          checkpoints_density: -1
          memory_type: MEM

              
train_config:
    max_epochs: 20
    batch_size_vae: 1
    batch_size_causal: 1
    epoch_vae_stop: 1
    epoch_causal_start: -1
    manual_sync: True

    seq_len_vae: 300
    seq_len_causal: 10000
    seg_len_vae: 300
    seg_len_causal: 1000

    lr_vae: 9.0e-5
    lr_causal: 2.0e-4
    lr_vae_decay_interval: 200000
    lr_causal_decay_interval: 10000
    lr_vae_start_step: 200000
    lr_causal_start_step: 10000

    data_path: "./data/maze_15_train/"
    save_model_path: "./train-record/causal-decoder-wm-DL-stage0/"
    max_save_iterations: 1000

    lossweight_policymodel: 0 # 0.01
    lossweight_worldmodel_raw: 0.9 # 0.9
    lossweight_worldmodel_latent: 0.09 # 0.09
    lossweight_l2: 1.0e-6

    use_amp: False
    use_scaler: False

    sigma_scheduler: 1000
    sigma_value:
        - 0.0
        - 0.01
        - 0.05

    lambda_scheduler: 1000
    lambda_value:
        - 0.0
        - 1.0e-8
        - 1.0e-7

test_config:
    batch_size_vae: 1
    batch_size_causal: 1
    data_path: "./data/maze_15_test/"
    epoch_vae_stop: 1
    epoch_causal_start: -1

    seq_len_vae: 300
    seq_len_causal: 10000
    seg_len_vae: 300
    seg_len_causal: 1000

    output: ./results15*15-causal-decoder-wm-DL-stage0/
